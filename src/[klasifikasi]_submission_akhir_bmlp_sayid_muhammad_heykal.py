# -*- coding: utf-8 -*-
"""[Klasifikasi]_Submission_Akhir_BMLP_Sayid_Muhammad_Heykal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/sayid-alt/student-performance-factor-analysis/blob/main/%5BKlasifikasi%5D_Submission_Akhir_BMLP_Sayid_Muhammad_Heykal.ipynb

# **1. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import urllib.request

import os
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB


from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, learning_curve, LearningCurveDisplay
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder

"""# **2. Memuat Dataset dari Hasil Clustering**

Memuat dataset hasil clustering dari file CSV ke dalam variabel DataFrame.
"""

RAW_FILE = 'https://raw.githubusercontent.com/sayid-alt/student-performance-factor-analysis/refs/heads/main/datasets/data-labeled.csv'

def fetch_clustered_data(link=RAW_FILE):
  download_link = urllib.request.urlretrieve(link, 'clustering.csv')
  print(f'\nsuccessfull download {download_link}')
  df = pd.read_csv('clustering.csv', encoding='utf-8', index_col=0)
  return df

df_clustered = fetch_clustered_data()

df_clustered.info()

df_clustered.head()

sns.countplot(x='cluster', data=df_clustered, palette='viridis')
plt.xlabel('Cluster')
plt.ylabel('Frequency')
plt.title('Frequency of Clusters')

"""## **2.1 Encoding**"""

categorical_cols = df_clustered.select_dtypes(include='object').columns

ordinal_features = ['parental_involvement',
                    'access_to_resources',
                    'motivation_level',
                    'family_income',
                    'teacher_quality',
                    'parental_education_level',
                    'distance_from_home',
                    'hours_studied_indicator',
                    'exam_score_indicator',
                    'tutoring_sessions_indicator']

nominal_features = df_clustered[categorical_cols].columns.difference(ordinal_features).tolist()

print(f'ordinal_features: {ordinal_features}\n')
print(f'nominal_features: {nominal_features}')

# store each ordinal column classes
class_one_ordinal = [col for col in ordinal_features if 'Medium' in df_clustered[col].unique()]
class_two_ordinal = [col for col in ordinal_features if 'College' in df_clustered[col].unique()]
class_three_ordinal = [col for col in ordinal_features if 'Moderate' in df_clustered[col].unique()]
class_four_ordinal = [col for col in ordinal_features if 'B' in df_clustered[col].unique()]

ordinal_classes = [class_one_ordinal, class_two_ordinal, class_three_ordinal, class_four_ordinal]

# set categories in each ordinal class
categories = [
    ['Low', 'Medium', 'High'],
    ['High School', 'College', 'Postgraduate'],
    ['Near', 'Moderate', 'Far'],
    ['D', 'C', 'B', 'A']
]

print(f'Classes in ordinal features:')
for i, class_i in enumerate(ordinal_classes):
    print(f'class {i+1}: {class_i}\n values: {categories[i]}\n')

# Encode the classes
for i, class_i in enumerate(ordinal_classes):
    enc = OrdinalEncoder(categories=[categories[i]]*len(class_i), dtype=np.int64)
    df_clustered[class_i] = enc.fit_transform(df_clustered[class_i])


# encode the nominal features
for col in nominal_features:
    dummies = pd.get_dummies(df_clustered[col], dtype=int, prefix=col)
    df_clustered = pd.concat([df_clustered, dummies], axis=1)
    df_clustered.drop(col, axis=1, inplace=True)

df_clustered.head()

"""# **3. Data Splitting**

Tahap Data Splitting bertujuan untuk memisahkan dataset menjadi dua bagian: data latih (training set) dan data uji (test set).
"""

X = df_clustered.drop(['cluster'], axis=1)
y = df_clustered['cluster']

print(f'shape of features: {X.shape}')
print(f'shape of target: {y.shape}')

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2, random_state=42)

portion_train = X_train.shape[0] / X.shape[0]
portion_test = X_test.shape[0] / X.shape[0]

print(f'shape of X_train: {X_train.shape}')
print(f'shape of y_train: {y_train.shape}')
print(f'shape of X_test: {X_test.shape}')
print(f'shape of y_test: {y_test.shape}')
print(f'percentage of train size: {portion_train:.2f}%')
print(f'percentage of test size: {portion_test:.2f}%')



"""# **4. Membangun Model Klasifikasi**

## **a. Membangun Model Klasifikasi**

Setelah memilih algoritma klasifikasi yang sesuai, langkah selanjutnya adalah melatih model menggunakan data latih.

Berikut adalah rekomendasi tahapannya.
1. Pilih algoritma klasifikasi yang sesuai, seperti Logistic Regression, Decision Tree, Random Forest, atau K-Nearest Neighbors (KNN).
2. Latih model menggunakan data latih.
"""

knn = KNeighborsClassifier()
lr = LogisticRegression(multi_class='ovr')
dt = DecisionTreeClassifier()
rf = RandomForestClassifier()
svm = SVC()
nb = MultinomialNB()

knn.fit(X_train, y_train)
lr.fit(X_train, y_train)
dt.fit(X_train, y_train)
rf.fit(X_train, y_train)
svm.fit(X_train, y_train)
nb.fit(X_train, y_train)

models = [knn, lr, dt, rf, svm, nb]
models

"""Tulis narasi atau penjelasan algoritma yang Anda gunakan.

Algoritma yang akan digunakan berjumlah 6 jenis, untuk melihat perbandingan-perbandingan yang lebih baik pada akurasinya. 6 Jenis teresbut antara lain:
1. `KNN`: Merupakan algoritma yang mengukur jarak antara titik prediksi dengan tetangga2 terdekatnya untuk mengidentifikasi kemiripan dan kelompok dari suatu titik, jenis algoritma in mudah untuk diintrepertasikan dan memungkinkan memakan memori lebih jika terlalu banyak data yang dilatih
2. `LogisticRegression`: Merupakan jenis algoritma yang mengklasifikasi hasil prediksi dengan nilai probabilitas antara 0 - 1, nilai *true* jika nilai probabilitas diatas nilai threshold yang ditentukan. Untuk model jenis ini digunakan parameter `multi_class = ovr` atau one vs rest. dengan membandingkan nilai binary antara satu kelas dan kelas yg lainnya yang bukan dari kelas tersebut.
3. `DecisionTreeClassifier`: Jenis Algoritma dengan memecah fitur menjadi 2 pilihan, dengan mengukur nilai impurity di setiap fitur. Sehingga menjadikan seperti bentuk pohon keputusan
4. `RandomForestClassifier`: Jenis gabungan dari algoritma decision tree dengan memilih random dari pada sub-tree.
5. `SVM`: Jenis algoritma yang memisahkan antar kelas dengan garis hyperplane yang memaksimalkan margin antara garis dan support vektor (titik terdekat dengan hyperplane dari setiap kelas)
6. `Naive Bayes`: jenis algoritma yang berdasarkan probabilitas teorema bayes. Dengan mengasumsikan fitur yang indpenden satu sama lain.

## **b. Evaluasi Model Klasifikasi**

Berikut adalah **rekomendasi** tahapannya.
1. Lakukan prediksi menggunakan data uji.
2. Hitung metrik evaluasi seperti Accuracy dan F1-Score (Opsional: Precision dan Recall).
3. Buat confusion matrix untuk melihat detail prediksi benar dan salah.
"""

def model_evaluate(model, X_test, y_test):
  y_pred = model.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred, average='weighted')
  cm = confusion_matrix(y_test, y_pred)

  return accuracy, f1, cm


model_names = []
accuracies = []
f1_scores = []
cms = []

for model in models:
  accuracy, f1, cm = model_evaluate(model, X_test, y_test)
  model_names.append(model.__class__.__name__)
  accuracies.append(accuracy)
  f1_scores.append(f1)
  cms.append(cm)


evals = pd.DataFrame({'Model': model_names,
                      'Accuracy': accuracies,
                      'F1 Score': f1_scores})


ncols = 3
nrows = len(evals) // ncols
fig, axes = plt.subplots(figsize=(15,4*nrows), nrows=nrows, ncols=ncols)
axes = axes.flatten()

for i, ax in enumerate(axes):
  sns.heatmap(cms[i], annot=True, fmt='d', ax=ax)
  ax.set_title(model_names[i], fontsize=12)

evals

"""Tulis hasil evaluasi algoritma yang digunakan, jika Anda menggunakan 2 algoritma, maka bandingkan hasilnya. <br><br>

**EVALUASI** <br>
Evaluasi dari 6 algoritma diatas menunjukkan keunggulan pada algoritma `Decision Tree`, `RandomForestClassifier`. Dengan mengatur semua parameter pada default value.

**STRATEGY**
Dari hasil diatas pembuatan model saat ini akan menggunakan Decision Tree.

## **c. Tuning Model Klasifikasi (Optional)**

Gunakan GridSearchCV, RandomizedSearchCV, atau metode lainnya untuk mencari kombinasi hyperparameter terbaik
"""

#Type your code here

"""## **d. Evaluasi Model Klasifikasi setelah Tuning (Optional)**

Berikut adalah rekomendasi tahapannya.
1. Gunakan model dengan hyperparameter terbaik.
2. Hitung ulang metrik evaluasi untuk melihat apakah ada peningkatan performa.
"""

#Type your code here

"""## **e. Analisis Hasil Evaluasi Model Klasifikasi**

Berikut adalah **rekomendasi** tahapannya.
1. Bandingkan hasil evaluasi sebelum dan setelah tuning (jika dilakukan).
2. Identifikasi kelemahan model, seperti:
  - Precision atau Recall rendah untuk kelas tertentu.
  - Apakah model mengalami overfitting atau underfitting?
3. Berikan rekomendasi tindakan lanjutan, seperti mengumpulkan data tambahan atau mencoba algoritma lain jika hasil belum memuaskan.
"""

from sklearn.metrics import recall_score, precision_score

y_train_pred = dt.predict(X_train)
y_test_pred = dt.predict(X_test)

recall = recall_score(y_test, y_test_pred, average='weighted')
precision = precision_score(y_test, y_test_pred, average='weighted')

print(f'train set accuracy: {accuracy_score(y_train, y_train_pred)}')
print(f'test set accuracy: {accuracy_score(y_test, y_test_pred)}')
print(f'Recall: {recall}')
print(f'Precision: {precision}')

train_size, train_score, test_score = learning_curve(dt, X_train, y_train, cv=2, verbose=3)

LearningCurveDisplay(train_sizes=train_size, train_scores=train_score, test_scores=test_score).plot()

train_score = np.mean(train_score, axis=1)
test_score = np.mean(test_score, axis=1)

plt.plot(train_size, train_score, label='train')
plt.plot(train_size, test_score, label='test')
plt.xlabel('Training Size')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Display grafik yang menunjukkan tidak ada nya overfit atau underfit pada model, mengidentifikasikan model yang memprediksi data secara baik dan dapat di generalisasikan dengan data yang baru"""

